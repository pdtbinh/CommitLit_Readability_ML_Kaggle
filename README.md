# CommitLit Readability Prize ML on Kaggle

This project includes the data, codes, and submissions for the CommonLit Readability Prize ML compettion 2021 on Kaggle. 

<b>Model solution</b>: Variants of RoBERTa (Robust Optimized BERT Pretraining Approach): 
* XMLRoBERTa
* RoBERTa-Base
* RoBERTa-Large 
* Combinations of those

<b>Result</b>: Best model has testing root mean squared error of 0.506 (0.060 more than first place's result, which is 0.446). 

