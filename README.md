# CommitLit Readability Prize ML on Kaggle

This project includes the data, codes, and submissions for the CommonLit Readability Prize ML compettion on Kaggle. 

<b>Model solution</b>: Fine-tuning on variants of RoBERTa (Robust Optimized BERT Pretraining Approach): 
* XMLRoBERTa
* RoBERTa-Base
* RoBERTa-Large (best model)
* Combinations of those

<b>Result</b>: Best model has testing root mean squared error of 0.506. Measurement unit is the final readabilty score.

