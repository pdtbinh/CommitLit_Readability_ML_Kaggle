{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gentle-calgary",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:17.971756Z",
     "iopub.status.busy": "2021-06-13T06:29:17.969808Z",
     "iopub.status.idle": "2021-06-13T06:29:25.153599Z",
     "shell.execute_reply": "2021-06-13T06:29:25.152461Z"
    },
    "papermill": {
     "duration": 7.20032,
     "end_time": "2021-06-13T06:29:25.153864",
     "exception": false,
     "start_time": "2021-06-13T06:29:17.953544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import RobertaTokenizerFast, TFXLMRobertaModel\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU, Dense\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Softmax, Conv1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "progressive-anniversary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:25.177878Z",
     "iopub.status.busy": "2021-06-13T06:29:25.176927Z",
     "iopub.status.idle": "2021-06-13T06:29:48.725690Z",
     "shell.execute_reply": "2021-06-13T06:29:48.726252Z"
    },
    "papermill": {
     "duration": 23.562611,
     "end_time": "2021-06-13T06:29:48.726435",
     "exception": false,
     "start_time": "2021-06-13T06:29:25.163824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at ../input/initial-xlmroberta-large/roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('../input/initial-tokenizer-large/tokenizer-large')\n",
    "roberta = TFXLMRobertaModel.from_pretrained('../input/initial-xlmroberta-large/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organized-mandate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:48.754124Z",
     "iopub.status.busy": "2021-06-13T06:29:48.753258Z",
     "iopub.status.idle": "2021-06-13T06:29:48.758177Z",
     "shell.execute_reply": "2021-06-13T06:29:48.757609Z"
    },
    "papermill": {
     "duration": 0.021635,
     "end_time": "2021-06-13T06:29:48.758319",
     "exception": false,
     "start_time": "2021-06-13T06:29:48.736684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize a bunch of texts\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    tok = tokenizer.batch_encode_plus(texts.tolist(), \n",
    "                                          padding='max_length', \n",
    "                                          max_length=max_len, \n",
    "                                          truncation=True, \n",
    "                                          return_tensors='np')\n",
    "    return tok['input_ids'], tok['attention_mask']\n",
    "\n",
    "# Prepare sets\n",
    "def prepare_sets(is_train_csv, tokenizer, max_len):\n",
    "    # Retrieve dataframes\n",
    "    df        = pd.read_csv('../input/commonlitreadabilityprize/' + is_train_csv)\n",
    "    \n",
    "    # Retrieve inputs: IDs and mask\n",
    "    excerpts  = df['excerpt']\n",
    "    tokenized = tokenize_texts(excerpts, tokenizer, max_len)\n",
    "    ids       = tokenized[0]\n",
    "    mask      = tokenized[1]\n",
    "    \n",
    "    # Retrieve labels\n",
    "    targets = None\n",
    "    if is_train_csv == 'train.csv':\n",
    "        targets   = np.array(df['target'])\n",
    "    \n",
    "    return ids, mask, targets, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "engaged-runner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:48.784993Z",
     "iopub.status.busy": "2021-06-13T06:29:48.784242Z",
     "iopub.status.idle": "2021-06-13T06:29:48.789263Z",
     "shell.execute_reply": "2021-06-13T06:29:48.788733Z"
    },
    "papermill": {
     "duration": 0.021402,
     "end_time": "2021-06-13T06:29:48.789395",
     "exception": false,
     "start_time": "2021-06-13T06:29:48.767993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to transform arrays to tensors\n",
    "def transform_to_tensors(x_train, x_val, y_train, y_val):\n",
    "    \n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(({'input_1': x_train[0], 'input_2': x_train[1]}, y_train))\n",
    "        .shuffle(2048)\n",
    "        .batch(8)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(({'input_1': x_val[0], 'input_2': x_val[1]}, y_val))\n",
    "        .batch(8)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controversial-divide",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:48.819578Z",
     "iopub.status.busy": "2021-06-13T06:29:48.817386Z",
     "iopub.status.idle": "2021-06-13T06:29:48.820435Z",
     "shell.execute_reply": "2021-06-13T06:29:48.821029Z"
    },
    "papermill": {
     "duration": 0.021687,
     "end_time": "2021-06-13T06:29:48.821216",
     "exception": false,
     "start_time": "2021-06-13T06:29:48.799529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(max_len, learning_rate):\n",
    "    # Inputs: IDs and mask\n",
    "    ids = Input(shape=(max_len,), dtype='int64')\n",
    "    mask = Input(shape=(max_len,), dtype='int64')\n",
    "    \n",
    "    # Processing\n",
    "    x = roberta(input_ids=ids, attention_mask=mask)['last_hidden_state']\n",
    "    x = x[:, 0, :]\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Output\n",
    "    pred = x\n",
    "    \n",
    "    # Model\n",
    "    model = Model(inputs=[ids, mask], outputs=pred)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = learning_rate),\n",
    "                  loss = [tf.keras.losses.MeanSquaredError()],\n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exciting-scott",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:48.846944Z",
     "iopub.status.busy": "2021-06-13T06:29:48.846160Z",
     "iopub.status.idle": "2021-06-13T06:29:48.850107Z",
     "shell.execute_reply": "2021-06-13T06:29:48.849572Z"
    },
    "papermill": {
     "duration": 0.018758,
     "end_time": "2021-06-13T06:29:48.850244",
     "exception": false,
     "start_time": "2021-06-13T06:29:48.831486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attended-calibration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:48.886932Z",
     "iopub.status.busy": "2021-06-13T06:29:48.884687Z",
     "iopub.status.idle": "2021-06-13T06:29:48.887879Z",
     "shell.execute_reply": "2021-06-13T06:29:48.888425Z"
    },
    "papermill": {
     "duration": 0.027926,
     "end_time": "2021-06-13T06:29:48.888605",
     "exception": false,
     "start_time": "2021-06-13T06:29:48.860679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(max_len, lr, batch, epoch_size, cp_path, seed):\n",
    "    # Prepare sets\n",
    "    train_set = prepare_sets('train.csv', tokenizer, max_len)\n",
    "    test_set = prepare_sets('test.csv', tokenizer, max_len)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(max_len, lr)\n",
    "    model.summary()\n",
    "    \n",
    "    # Randomize\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(cp_path, \n",
    "                                                    monitor = 'val_root_mean_squared_error', \n",
    "                                                    verbose = 2, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "    \n",
    "    reduce_lr=ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\",\n",
    "                                factor=0.2,\n",
    "                                patience=5,\n",
    "                                min_lr=1e-8)\n",
    "    \n",
    "    early_stopping=EarlyStopping(monitor=\"val_root_mean_squared_error\",\n",
    "                                 min_delta=0,\n",
    "                                 patience=5,\n",
    "                                 verbose=2,\n",
    "                                 mode=\"min\",\n",
    "                                 restore_best_weights=True)\n",
    "    \n",
    "    # Use 5-fold CV\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    out_of_fold_pred = np.zeros((len(train_set[2])))\n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_set[3])):\n",
    "        print('\\nFold', fold+1, '*'*50)\n",
    "        \n",
    "        sets = transform_to_tensors(x_train = (train_set[0][trn_ind], train_set[1][trn_ind]),\n",
    "                                    x_val = (train_set[0][val_ind], train_set[1][val_ind]),\n",
    "                                    y_train = train_set[2][trn_ind],\n",
    "                                    y_val = train_set[2][val_ind])\n",
    "    \n",
    "        model.fit(sets[0], \n",
    "                  batch_size=batch, epochs=epoch_size,\n",
    "                  validation_data = sets[1],\n",
    "                  callbacks = [checkpoint, reduce_lr, early_stopping])\n",
    "    \n",
    "        model.load_weights(cp_path)\n",
    "        cv_pred = model.predict(sets[1])\n",
    "        out_of_fold_pred[val_ind] = cv_pred.reshape(-1)\n",
    "    \n",
    "    cv_rmse = np.sqrt(mean_squared_error(train_set[2], out_of_fold_pred))\n",
    "    print('Out-of-fold Root Mean Square Error is:', cv_rmse)\n",
    "    \n",
    "    # Predict test\n",
    "    test_pred = model.predict([test_set[0], test_set[1]], test_set[2])\n",
    "    \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "elder-clinton",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:29:48.916599Z",
     "iopub.status.busy": "2021-06-13T06:29:48.915954Z",
     "iopub.status.idle": "2021-06-13T09:42:42.361313Z",
     "shell.execute_reply": "2021-06-13T09:42:42.360786Z"
    },
    "papermill": {
     "duration": 11573.462504,
     "end_time": "2021-06-13T09:42:42.361464",
     "exception": false,
     "start_time": "2021-06-13T06:29:48.898960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tfxlm_roberta_model (TFXLMRober TFBaseModelOutputWit 355359744   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 1024)         0           tfxlm_roberta_model[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1024)         0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,360,769\n",
      "Trainable params: 355,360,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Fold 1 **************************************************\n",
      "Epoch 1/8\n",
      "284/284 [==============================] - 350s 1s/step - loss: 2.1086 - root_mean_squared_error: 1.4360 - val_loss: 0.4736 - val_root_mean_squared_error: 0.6882\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.68820, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 2/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 1.1438 - root_mean_squared_error: 1.0693 - val_loss: 0.3916 - val_root_mean_squared_error: 0.6258\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.68820 to 0.62578, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 3/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 1.0045 - root_mean_squared_error: 1.0011 - val_loss: 0.3504 - val_root_mean_squared_error: 0.5920\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.62578 to 0.59196, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 4/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.7879 - root_mean_squared_error: 0.8867 - val_loss: 0.3380 - val_root_mean_squared_error: 0.5814\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.59196 to 0.58136, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 5/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.5534 - root_mean_squared_error: 0.7428 - val_loss: 0.3434 - val_root_mean_squared_error: 0.5860\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.58136\n",
      "Epoch 6/8\n",
      "284/284 [==============================] - 305s 1s/step - loss: 0.4257 - root_mean_squared_error: 0.6519 - val_loss: 0.3281 - val_root_mean_squared_error: 0.5728\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.58136 to 0.57276, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 7/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.3355 - root_mean_squared_error: 0.5788 - val_loss: 0.4105 - val_root_mean_squared_error: 0.6407\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.57276\n",
      "Epoch 8/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.2567 - root_mean_squared_error: 0.5062 - val_loss: 0.4033 - val_root_mean_squared_error: 0.6350\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.57276\n",
      "\n",
      "Fold 2 **************************************************\n",
      "Epoch 1/8\n",
      "284/284 [==============================] - 305s 1s/step - loss: 0.4035 - root_mean_squared_error: 0.6352 - val_loss: 0.3786 - val_root_mean_squared_error: 0.6153\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.57276\n",
      "Epoch 2/8\n",
      "284/284 [==============================] - 305s 1s/step - loss: 0.3347 - root_mean_squared_error: 0.5786 - val_loss: 0.2322 - val_root_mean_squared_error: 0.4819\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.57276 to 0.48191, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 3/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.2557 - root_mean_squared_error: 0.5056 - val_loss: 0.4172 - val_root_mean_squared_error: 0.6459\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.48191\n",
      "Epoch 4/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.2021 - root_mean_squared_error: 0.4496 - val_loss: 0.2770 - val_root_mean_squared_error: 0.5263\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.48191\n",
      "Epoch 5/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1642 - root_mean_squared_error: 0.4052 - val_loss: 0.2493 - val_root_mean_squared_error: 0.4993\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.48191\n",
      "Epoch 6/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1380 - root_mean_squared_error: 0.3715 - val_loss: 0.2640 - val_root_mean_squared_error: 0.5138\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.48191\n",
      "Epoch 7/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1205 - root_mean_squared_error: 0.3472 - val_loss: 0.3127 - val_root_mean_squared_error: 0.5592\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.48191\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Fold 3 **************************************************\n",
      "Epoch 1/8\n",
      "284/284 [==============================] - 307s 1s/step - loss: 0.2239 - root_mean_squared_error: 0.4732 - val_loss: 0.2514 - val_root_mean_squared_error: 0.5014\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.48191\n",
      "Epoch 2/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.2006 - root_mean_squared_error: 0.4479 - val_loss: 0.2452 - val_root_mean_squared_error: 0.4952\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.48191\n",
      "Epoch 3/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1864 - root_mean_squared_error: 0.4317 - val_loss: 0.1783 - val_root_mean_squared_error: 0.4222\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.48191 to 0.42223, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 4/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1713 - root_mean_squared_error: 0.4139 - val_loss: 0.1500 - val_root_mean_squared_error: 0.3874\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.42223 to 0.38736, saving model to cp_xml_roberta_large.h5\n",
      "Epoch 5/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1558 - root_mean_squared_error: 0.3947 - val_loss: 0.2812 - val_root_mean_squared_error: 0.5303\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 6/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1330 - root_mean_squared_error: 0.3647 - val_loss: 0.3086 - val_root_mean_squared_error: 0.5555\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 7/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1210 - root_mean_squared_error: 0.3479 - val_loss: 0.3282 - val_root_mean_squared_error: 0.5729\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 8/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1141 - root_mean_squared_error: 0.3378 - val_loss: 0.2917 - val_root_mean_squared_error: 0.5400\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.38736\n",
      "\n",
      "Fold 4 **************************************************\n",
      "Epoch 1/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1596 - root_mean_squared_error: 0.3995 - val_loss: 0.2786 - val_root_mean_squared_error: 0.5278\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 2/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1460 - root_mean_squared_error: 0.3820 - val_loss: 0.2836 - val_root_mean_squared_error: 0.5326\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 3/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1279 - root_mean_squared_error: 0.3576 - val_loss: 0.1795 - val_root_mean_squared_error: 0.4236\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 4/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1281 - root_mean_squared_error: 0.3579 - val_loss: 0.3820 - val_root_mean_squared_error: 0.6181\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 5/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1140 - root_mean_squared_error: 0.3377 - val_loss: 0.4061 - val_root_mean_squared_error: 0.6373\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 6/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1044 - root_mean_squared_error: 0.3232 - val_loss: 0.2848 - val_root_mean_squared_error: 0.5336\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 7/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.0897 - root_mean_squared_error: 0.2995 - val_loss: 0.2825 - val_root_mean_squared_error: 0.5315\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 8/8\n",
      "284/284 [==============================] - 307s 1s/step - loss: 0.0912 - root_mean_squared_error: 0.3020 - val_loss: 0.2324 - val_root_mean_squared_error: 0.4820\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.38736\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "Fold 5 **************************************************\n",
      "Epoch 1/8\n",
      "284/284 [==============================] - 307s 1s/step - loss: 0.1511 - root_mean_squared_error: 0.3887 - val_loss: 0.1825 - val_root_mean_squared_error: 0.4272\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 2/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1531 - root_mean_squared_error: 0.3912 - val_loss: 0.3023 - val_root_mean_squared_error: 0.5498\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 3/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1441 - root_mean_squared_error: 0.3795 - val_loss: 0.2284 - val_root_mean_squared_error: 0.4779\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 4/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1430 - root_mean_squared_error: 0.3782 - val_loss: 0.2655 - val_root_mean_squared_error: 0.5153\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 5/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1407 - root_mean_squared_error: 0.3751 - val_loss: 0.2200 - val_root_mean_squared_error: 0.4690\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.38736\n",
      "Epoch 6/8\n",
      "284/284 [==============================] - 306s 1s/step - loss: 0.1322 - root_mean_squared_error: 0.3635 - val_loss: 0.2461 - val_root_mean_squared_error: 0.4961\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.38736\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Out-of-fold Root Mean Square Error is: 0.42857403177364467\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 300\n",
    "LR = 1e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 8\n",
    "CP_PATH = 'cp_xml_roberta_large.h5'\n",
    "SEED = 2021\n",
    "\n",
    "test_pred = train_model(MAX_LEN, LR, BATCH_SIZE, EPOCHS, CP_PATH, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forced-heath",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:42:50.024406Z",
     "iopub.status.busy": "2021-06-13T09:42:50.023322Z",
     "iopub.status.idle": "2021-06-13T09:42:50.310682Z",
     "shell.execute_reply": "2021-06-13T09:42:50.309983Z"
    },
    "papermill": {
     "duration": 4.001688,
     "end_time": "2021-06-13T09:42:50.310856",
     "exception": false,
     "start_time": "2021-06-13T09:42:46.309168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "pred_df = {'id': test_df['id'], 'target': test_pred.reshape(-1)}\n",
    "pd.DataFrame(pred_df).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-invite",
   "metadata": {
    "papermill": {
     "duration": 3.875803,
     "end_time": "2021-06-13T09:42:58.269665",
     "exception": false,
     "start_time": "2021-06-13T09:42:54.393862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11635.224765,
   "end_time": "2021-06-13T09:43:04.839496",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-13T06:29:09.614731",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
