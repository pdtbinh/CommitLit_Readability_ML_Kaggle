{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-demographic",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:00.604763Z",
     "iopub.status.busy": "2021-06-12T10:01:00.604094Z",
     "iopub.status.idle": "2021-06-12T10:01:06.255568Z",
     "shell.execute_reply": "2021-06-12T10:01:06.254644Z"
    },
    "papermill": {
     "duration": 5.671012,
     "end_time": "2021-06-12T10:01:06.255730",
     "exception": false,
     "start_time": "2021-06-12T10:01:00.584718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU, Dense\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Softmax, Conv1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "received-agency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:06.272880Z",
     "iopub.status.busy": "2021-06-12T10:01:06.272386Z",
     "iopub.status.idle": "2021-06-12T10:01:28.972659Z",
     "shell.execute_reply": "2021-06-12T10:01:28.973146Z"
    },
    "papermill": {
     "duration": 22.710448,
     "end_time": "2021-06-12T10:01:28.973339",
     "exception": false,
     "start_time": "2021-06-12T10:01:06.262891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/initial-roberta-large/roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('../input/initial-tokenizer-large/tokenizer-large')\n",
    "roberta = TFRobertaModel.from_pretrained('../input/initial-roberta-large/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-discovery",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:28.997109Z",
     "iopub.status.busy": "2021-06-12T10:01:28.995807Z",
     "iopub.status.idle": "2021-06-12T10:01:28.998323Z",
     "shell.execute_reply": "2021-06-12T10:01:28.998740Z"
    },
    "papermill": {
     "duration": 0.017206,
     "end_time": "2021-06-12T10:01:28.998886",
     "exception": false,
     "start_time": "2021-06-12T10:01:28.981680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize a bunch of texts\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    tok = tokenizer.batch_encode_plus(texts.tolist(), \n",
    "                                          padding='max_length', \n",
    "                                          max_length=max_len, \n",
    "                                          truncation=True, \n",
    "                                          return_tensors='np')\n",
    "    return tok['input_ids'], tok['attention_mask']\n",
    "\n",
    "# Prepare sets\n",
    "def prepare_sets(is_train_csv, tokenizer, max_len):\n",
    "    # Retrieve dataframes\n",
    "    df = pd.read_csv('../input/commonlitreadabilityprize/' + is_train_csv)\n",
    "    \n",
    "    # Retrieve inputs: IDs and mask\n",
    "    excerpts  = df['excerpt']\n",
    "    tokenized = tokenize_texts(excerpts, tokenizer, max_len)\n",
    "    ids       = tokenized[0]\n",
    "    mask      = tokenized[1]\n",
    "    \n",
    "    # Retrieve labels\n",
    "    targets = None\n",
    "    if is_train_csv == 'train.csv':\n",
    "        targets   = np.array(df['target'])\n",
    "    \n",
    "    return ids, mask, targets, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "transparent-terrorist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:29.019688Z",
     "iopub.status.busy": "2021-06-12T10:01:29.019178Z",
     "iopub.status.idle": "2021-06-12T10:01:29.022545Z",
     "shell.execute_reply": "2021-06-12T10:01:29.022958Z"
    },
    "papermill": {
     "duration": 0.016526,
     "end_time": "2021-06-12T10:01:29.023090",
     "exception": false,
     "start_time": "2021-06-12T10:01:29.006564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to transform arrays to tensors\n",
    "def transform_to_tensors(x_train, x_val, y_train, y_val):\n",
    "    \n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(({'input_1': x_train[0], 'input_2': x_train[1]}, y_train))\n",
    "        .shuffle(2048)\n",
    "        .batch(8)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(({'input_1': x_val[0], 'input_2': x_val[1]}, y_val))\n",
    "        .batch(8)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infrared-yeast",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:29.043989Z",
     "iopub.status.busy": "2021-06-12T10:01:29.043399Z",
     "iopub.status.idle": "2021-06-12T10:01:29.047221Z",
     "shell.execute_reply": "2021-06-12T10:01:29.046807Z"
    },
    "papermill": {
     "duration": 0.016718,
     "end_time": "2021-06-12T10:01:29.047320",
     "exception": false,
     "start_time": "2021-06-12T10:01:29.030602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(max_len, learning_rate):\n",
    "    # Inputs: IDs and mask\n",
    "    ids = Input(shape=(max_len,), dtype='int64')\n",
    "    mask = Input(shape=(max_len,), dtype='int64')\n",
    "    \n",
    "    # Processing\n",
    "    x = roberta(input_ids=ids, attention_mask=mask)['last_hidden_state']\n",
    "    x = x[:, 0, :]\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Output\n",
    "    pred = x\n",
    "    \n",
    "    # Model\n",
    "    model = Model(inputs=[ids, mask], outputs=pred)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = learning_rate),\n",
    "                  loss = [tf.keras.losses.MeanSquaredError()],\n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "radical-wiring",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:29.067343Z",
     "iopub.status.busy": "2021-06-12T10:01:29.066079Z",
     "iopub.status.idle": "2021-06-12T10:01:29.068392Z",
     "shell.execute_reply": "2021-06-12T10:01:29.068838Z"
    },
    "papermill": {
     "duration": 0.01404,
     "end_time": "2021-06-12T10:01:29.068958",
     "exception": false,
     "start_time": "2021-06-12T10:01:29.054918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "automatic-algeria",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:29.095193Z",
     "iopub.status.busy": "2021-06-12T10:01:29.091400Z",
     "iopub.status.idle": "2021-06-12T10:01:29.097544Z",
     "shell.execute_reply": "2021-06-12T10:01:29.097146Z"
    },
    "papermill": {
     "duration": 0.021144,
     "end_time": "2021-06-12T10:01:29.097644",
     "exception": false,
     "start_time": "2021-06-12T10:01:29.076500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(max_len, lr, batch, epoch_size, cp_path, seed):\n",
    "    # Prepare sets\n",
    "    train_set = prepare_sets('train.csv', tokenizer, max_len)\n",
    "    test_set = prepare_sets('test.csv', tokenizer, max_len)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(max_len, lr)\n",
    "    model.summary()\n",
    "    \n",
    "    # Randomize\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(cp_path, \n",
    "                                                    monitor = 'val_root_mean_squared_error', \n",
    "                                                    verbose = 2, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "    \n",
    "    reduce_lr=ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\",\n",
    "                                factor=0.2,\n",
    "                                patience=5,\n",
    "                                min_lr=1e-8)\n",
    "    \n",
    "    early_stopping=EarlyStopping(monitor=\"val_root_mean_squared_error\",\n",
    "                                 min_delta=0,\n",
    "                                 patience=5,\n",
    "                                 verbose=2,\n",
    "                                 mode=\"min\",\n",
    "                                 restore_best_weights=True)\n",
    "    \n",
    "    # Use k-fold CV\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    out_of_fold_pred = np.zeros((len(train_set[2])))\n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_set[3])):\n",
    "        print('\\nFold', fold+1, '*'*50)\n",
    "        \n",
    "        sets = transform_to_tensors(x_train = (train_set[0][trn_ind], train_set[1][trn_ind]),\n",
    "                                    x_val = (train_set[0][val_ind], train_set[1][val_ind]),\n",
    "                                    y_train = train_set[2][trn_ind],\n",
    "                                    y_val = train_set[2][val_ind])\n",
    "    \n",
    "        model.fit(sets[0], \n",
    "                  batch_size=batch, epochs=epoch_size,\n",
    "                  validation_data = sets[1],\n",
    "                  callbacks = [checkpoint, reduce_lr, early_stopping])\n",
    "    \n",
    "        model.load_weights(cp_path)\n",
    "        cv_pred = model.predict(sets[1])\n",
    "        out_of_fold_pred[val_ind] = cv_pred.reshape(-1)\n",
    "    \n",
    "    cv_rmse = np.sqrt(mean_squared_error(train_set[2], out_of_fold_pred))\n",
    "    print('Out-of-fold Root Mean Square Error is:', cv_rmse)\n",
    "    \n",
    "    # Predict test\n",
    "    test_pred = model.predict([test_set[0], test_set[1]], test_set[2])\n",
    "    \n",
    "    return test_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advance-newfoundland",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:01:29.118008Z",
     "iopub.status.busy": "2021-06-12T10:01:29.117517Z",
     "iopub.status.idle": "2021-06-12T11:56:30.520666Z",
     "shell.execute_reply": "2021-06-12T11:56:30.520227Z"
    },
    "papermill": {
     "duration": 6901.415331,
     "end_time": "2021-06-12T11:56:30.520820",
     "exception": false,
     "start_time": "2021-06-12T10:01:29.105489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 355359744   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 1024)         0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1024)         0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,360,769\n",
      "Trainable params: 355,360,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Fold 1 **************************************************\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 301s 960ms/step - loss: 1.5910 - root_mean_squared_error: 1.2594 - val_loss: 0.4761 - val_root_mean_squared_error: 0.6900\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.68999, saving model to cp_roberta_large.h5\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.9937 - root_mean_squared_error: 0.9953 - val_loss: 0.3360 - val_root_mean_squared_error: 0.5797\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.68999 to 0.57969, saving model to cp_roberta_large.h5\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.7642 - root_mean_squared_error: 0.8740 - val_loss: 0.3459 - val_root_mean_squared_error: 0.5881\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.57969\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.6781 - root_mean_squared_error: 0.8233 - val_loss: 0.2982 - val_root_mean_squared_error: 0.5461\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.57969 to 0.54612, saving model to cp_roberta_large.h5\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 267s 940ms/step - loss: 0.5335 - root_mean_squared_error: 0.7300 - val_loss: 0.6895 - val_root_mean_squared_error: 0.8303\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.54612\n",
      "\n",
      "Fold 2 **************************************************\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 267s 940ms/step - loss: 0.5375 - root_mean_squared_error: 0.7331 - val_loss: 0.3619 - val_root_mean_squared_error: 0.6015\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.54612\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 267s 940ms/step - loss: 0.6362 - root_mean_squared_error: 0.7976 - val_loss: 0.4885 - val_root_mean_squared_error: 0.6989\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.54612\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 267s 939ms/step - loss: 0.4338 - root_mean_squared_error: 0.6586 - val_loss: 0.6530 - val_root_mean_squared_error: 0.8081\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.54612\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 267s 940ms/step - loss: 0.3527 - root_mean_squared_error: 0.5939 - val_loss: 0.2220 - val_root_mean_squared_error: 0.4712\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.54612 to 0.47118, saving model to cp_roberta_large.h5\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.2462 - root_mean_squared_error: 0.4962 - val_loss: 0.3552 - val_root_mean_squared_error: 0.5960\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.47118\n",
      "\n",
      "Fold 3 **************************************************\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.2934 - root_mean_squared_error: 0.5417 - val_loss: 0.5717 - val_root_mean_squared_error: 0.7561\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.47118\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.2141 - root_mean_squared_error: 0.4627 - val_loss: 0.4259 - val_root_mean_squared_error: 0.6526\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.47118\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 267s 940ms/step - loss: 0.1569 - root_mean_squared_error: 0.3962 - val_loss: 0.4836 - val_root_mean_squared_error: 0.6954\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.47118\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 267s 940ms/step - loss: 0.1172 - root_mean_squared_error: 0.3424 - val_loss: 0.3832 - val_root_mean_squared_error: 0.6190\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.47118\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 267s 941ms/step - loss: 0.1052 - root_mean_squared_error: 0.3243 - val_loss: 0.3534 - val_root_mean_squared_error: 0.5945\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.47118\n",
      "\n",
      "Fold 4 **************************************************\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 267s 942ms/step - loss: 0.3233 - root_mean_squared_error: 0.5686 - val_loss: 0.3508 - val_root_mean_squared_error: 0.5923\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.47118\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 267s 942ms/step - loss: 0.2162 - root_mean_squared_error: 0.4650 - val_loss: 0.2115 - val_root_mean_squared_error: 0.4599\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.47118 to 0.45989, saving model to cp_roberta_large.h5\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 267s 942ms/step - loss: 0.1535 - root_mean_squared_error: 0.3918 - val_loss: 0.2576 - val_root_mean_squared_error: 0.5075\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.45989\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 268s 942ms/step - loss: 0.1076 - root_mean_squared_error: 0.3281 - val_loss: 0.1769 - val_root_mean_squared_error: 0.4206\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.45989 to 0.42062, saving model to cp_roberta_large.h5\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 268s 942ms/step - loss: 0.1047 - root_mean_squared_error: 0.3236 - val_loss: 0.1641 - val_root_mean_squared_error: 0.4051\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.42062 to 0.40506, saving model to cp_roberta_large.h5\n",
      "\n",
      "Fold 5 **************************************************\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 268s 943ms/step - loss: 0.1258 - root_mean_squared_error: 0.3547 - val_loss: 0.1443 - val_root_mean_squared_error: 0.3798\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from 0.40506 to 0.37984, saving model to cp_roberta_large.h5\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 268s 943ms/step - loss: 0.1018 - root_mean_squared_error: 0.3190 - val_loss: 0.1256 - val_root_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.37984 to 0.35434, saving model to cp_roberta_large.h5\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 268s 943ms/step - loss: 0.0801 - root_mean_squared_error: 0.2830 - val_loss: 0.5443 - val_root_mean_squared_error: 0.7378\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.35434\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 268s 943ms/step - loss: 0.0762 - root_mean_squared_error: 0.2760 - val_loss: 0.0926 - val_root_mean_squared_error: 0.3044\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.35434 to 0.30435, saving model to cp_roberta_large.h5\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 268s 943ms/step - loss: 0.0680 - root_mean_squared_error: 0.2608 - val_loss: 0.3915 - val_root_mean_squared_error: 0.6257\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.30435\n",
      "Out-of-fold Root Mean Square Error is: 0.42614817251952614\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "LR = 1e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "CP_PATH = 'cp_roberta_large.h5'\n",
    "SEED = 501\n",
    "\n",
    "result = train_model(MAX_LEN, LR, BATCH_SIZE, EPOCHS, CP_PATH, SEED)\n",
    "test_pred = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "seven-nicholas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T11:56:34.330917Z",
     "iopub.status.busy": "2021-06-12T11:56:34.330339Z",
     "iopub.status.idle": "2021-06-12T11:56:34.526922Z",
     "shell.execute_reply": "2021-06-12T11:56:34.526441Z"
    },
    "papermill": {
     "duration": 2.210245,
     "end_time": "2021-06-12T11:56:34.527051",
     "exception": false,
     "start_time": "2021-06-12T11:56:32.316806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "pred_df = {'id': test_df['id'], 'target': test_pred.reshape(-1)}\n",
    "pd.DataFrame(pred_df).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-black",
   "metadata": {
    "papermill": {
     "duration": 1.788774,
     "end_time": "2021-06-12T11:56:38.588039",
     "exception": false,
     "start_time": "2021-06-12T11:56:36.799265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6949.055248,
   "end_time": "2021-06-12T11:56:43.129721",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-12T10:00:54.074473",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
