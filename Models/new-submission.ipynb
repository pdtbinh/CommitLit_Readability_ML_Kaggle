{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "killing-appendix",
   "metadata": {
    "id": "Iqd38A53s5K-",
    "papermill": {
     "duration": 0.011769,
     "end_time": "2021-06-27T22:07:15.474259",
     "exception": false,
     "start_time": "2021-06-27T22:07:15.462490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Installs & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laughing-emerald",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:15.511448Z",
     "iopub.status.busy": "2021-06-27T22:07:15.510887Z",
     "iopub.status.idle": "2021-06-27T22:07:48.638076Z",
     "shell.execute_reply": "2021-06-27T22:07:48.637482Z",
     "shell.execute_reply.started": "2021-06-27T22:01:12.504399Z"
    },
    "id": "aqVq7alHq5eb",
    "outputId": "b538eb1f-5864-48ab-a370-0f15e6857f1a",
    "papermill": {
     "duration": 33.153179,
     "end_time": "2021-06-27T22:07:48.638253",
     "exception": false,
     "start_time": "2021-06-27T22:07:15.485074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.5.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.59.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.3.17)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "#!pip install tensorflow-addons\n",
    "\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU, Dense, RepeatVector, Dense, Activation, Lambda, Softmax, Conv1D, LayerNormalization, Softmax, Multiply, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-probe",
   "metadata": {
    "id": "3yAzufptsYN1",
    "papermill": {
     "duration": 0.011744,
     "end_time": "2021-06-27T22:07:48.662576",
     "exception": false,
     "start_time": "2021-06-27T22:07:48.650832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Transformers:** RoBERTa-Large & RoBERTa-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-latvia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:48.689701Z",
     "iopub.status.busy": "2021-06-27T22:07:48.689150Z",
     "iopub.status.idle": "2021-06-27T22:07:49.093684Z",
     "shell.execute_reply": "2021-06-27T22:07:49.094147Z",
     "shell.execute_reply.started": "2021-06-27T22:01:45.396480Z"
    },
    "id": "gin3HwEYsWRY",
    "outputId": "c41fa36e-1822-4eea-aa25-4b733209a8a3",
    "papermill": {
     "duration": 0.420026,
     "end_time": "2021-06-27T22:07:49.094405",
     "exception": false,
     "start_time": "2021-06-27T22:07:48.674379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pretrained RoBERTa-Large\n",
    "tokenizer_large = RobertaTokenizerFast.from_pretrained('../input/initial-tokenizer-large/tokenizer-large')\n",
    "\n",
    "# Pretrained RoBERTa-Base\n",
    "tokenizer_base = RobertaTokenizerFast.from_pretrained('../input/initial-tokenizer-base/tokenizer-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-maldives",
   "metadata": {
    "id": "Km2gAOS_tLOc",
    "papermill": {
     "duration": 0.011816,
     "end_time": "2021-06-27T22:07:49.118288",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.106472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Preprocessing**\n",
    "\n",
    "Features:\n",
    "1. RoBERTa-Large tokenizers\n",
    "2. RoBERTa-Base tokenizers\n",
    "3. Rarity per word (GloVe)\n",
    "5. Character per word\n",
    "4. Word per sentence\n",
    "\n",
    "Labels:\n",
    "1. Target score - Readbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coastal-lending",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.146215Z",
     "iopub.status.busy": "2021-06-27T22:07:49.145658Z",
     "iopub.status.idle": "2021-06-27T22:07:49.149593Z",
     "shell.execute_reply": "2021-06-27T22:07:49.149181Z",
     "shell.execute_reply.started": "2021-06-27T22:01:45.904237Z"
    },
    "id": "RB1LuhILIiSy",
    "papermill": {
     "duration": 0.019593,
     "end_time": "2021-06-27T22:07:49.149696",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.130103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature 1+2: \n",
    "# RoBERTa tokenizers\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    tok = tokenizer.batch_encode_plus(texts.tolist(), \n",
    "                                      padding='max_length', \n",
    "                                      max_length=max_len, \n",
    "                                      truncation=True, \n",
    "                                      return_tensors='np')\n",
    "    return tok['input_ids'], tok['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "signed-front",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.179754Z",
     "iopub.status.busy": "2021-06-27T22:07:49.178538Z",
     "iopub.status.idle": "2021-06-27T22:07:49.180799Z",
     "shell.execute_reply": "2021-06-27T22:07:49.181240Z",
     "shell.execute_reply.started": "2021-06-27T22:01:45.914443Z"
    },
    "id": "o0CL0ixDTqSv",
    "papermill": {
     "duration": 0.019822,
     "end_time": "2021-06-27T22:07:49.181360",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.161538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "\n",
    "def prepare_sets(address, max_len):\n",
    "    # Retrieve dataframes\n",
    "    df        = pd.read_csv(address)\n",
    "    \n",
    "    # Retrieve inputs: IDs and mask\n",
    "    excerpts  = df['excerpt']\n",
    "\n",
    "    # Feature 1\n",
    "    tokenized_large = tokenize_texts(excerpts, tokenizer_large, max_len)\n",
    "    ids_large       = tokenized_large[0]\n",
    "    msk_large       = tokenized_large[1]\n",
    "\n",
    "    # Feature 2\n",
    "    tokenized_base = tokenize_texts(excerpts, tokenizer_base, max_len)\n",
    "    ids_base       = tokenized_base[0]\n",
    "    msk_base       = tokenized_base[1]\n",
    "\n",
    "    # Retrieve labels\n",
    "    targets = None\n",
    "    if address == 'train.csv':\n",
    "        targets = np.array(df['target'])\n",
    "  \n",
    "    return ids_large, msk_large, ids_base, msk_base, targets, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compressed-stuart",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.212066Z",
     "iopub.status.busy": "2021-06-27T22:07:49.210889Z",
     "iopub.status.idle": "2021-06-27T22:07:49.213647Z",
     "shell.execute_reply": "2021-06-27T22:07:49.213186Z",
     "shell.execute_reply.started": "2021-06-27T22:01:45.928664Z"
    },
    "id": "kqls4LobtQIS",
    "papermill": {
     "duration": 0.020739,
     "end_time": "2021-06-27T22:07:49.213744",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.193005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features + Labels:\n",
    "# Combine all features and labels into tensors to feed into model\n",
    "# It is important that the input order remains the same until this point\n",
    "\n",
    "def transform_to_tensors(x_trn, y_trn, x_val, y_val, seed, batch_size):\n",
    "\n",
    "    train_dataset = (tf.data.Dataset\n",
    "                     .from_tensor_slices(({'input_roberta_large_ids': x_trn[0], \n",
    "                                         'input_roberta_large_msk': x_trn[1],\n",
    "                                         'input_roberta_base_ids':  x_trn[2], \n",
    "                                         'input_roberta_base_msk':  x_trn[3]},\n",
    "                                        y_trn))\n",
    "                     .shuffle(seed)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    valid_dataset = (tf.data.Dataset\n",
    "                     .from_tensor_slices(({'input_roberta_large_ids': x_val[0], \n",
    "                                         'input_roberta_large_msk': x_val[1],\n",
    "                                         'input_roberta_base_ids':  x_val[2],\n",
    "                                         'input_roberta_base_msk':  x_val[3]},\n",
    "                                        y_val))\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-sustainability",
   "metadata": {
    "id": "rirHoKodufg4",
    "papermill": {
     "duration": 0.011939,
     "end_time": "2021-06-27T22:07:49.237297",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.225358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foreign-corpus",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.274732Z",
     "iopub.status.busy": "2021-06-27T22:07:49.274070Z",
     "iopub.status.idle": "2021-06-27T22:07:49.277298Z",
     "shell.execute_reply": "2021-06-27T22:07:49.276893Z",
     "shell.execute_reply.started": "2021-06-27T22:04:59.318066Z"
    },
    "id": "Rn_6L_Ztulbv",
    "papermill": {
     "duration": 0.028268,
     "end_time": "2021-06-27T22:07:49.277398",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.249130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(max_len, learning_rate, dc):\n",
    "    \n",
    "    ################\n",
    "    #    Input     #\n",
    "    ################\n",
    "    roberta_large = TFRobertaModel.from_pretrained('../input/initial-roberta-large/roberta-large')\n",
    "    roberta_base = TFRobertaModel.from_pretrained('../input/initial-roberta-base/roberta-base')\n",
    "\n",
    "    # RoBERTa-Large\n",
    "    ids_large = Input(shape=(max_len,), dtype='int64', name='input_roberta_large_ids')\n",
    "    msk_large = Input(shape=(max_len,), dtype='int64', name='input_roberta_large_msk')\n",
    "\n",
    "    # RoBERTa-Base\n",
    "    ids_base = Input(shape=(max_len,), dtype='int64', name='input_roberta_base_ids')\n",
    "    msk_base = Input(shape=(max_len,), dtype='int64', name='input_roberta_base_msk')\n",
    "\n",
    "    ################\n",
    "    #  Processing  #\n",
    "    ################\n",
    "\n",
    "    dropout_rate = 0\n",
    "\n",
    "    # RoBERTa-Large\n",
    "    \n",
    "    ids_large_1 = ids_large[:, :128]\n",
    "    msk_large_1 = msk_large[:, :128]\n",
    "    x_large_1 = roberta_large(input_ids = ids_large_1, attention_mask = msk_large_1)['last_hidden_state']\n",
    "    x_large_1 = x_large_1[:, 0, :]\n",
    "    x_large_1 = Dropout(dropout_rate)(x_large_1)\n",
    "    x_large_1 = Dense(1, activation='linear', kernel_regularizer=regularizers.l2(1e-2))(x_large_1)\n",
    "    \n",
    "    ids_large_2 = ids_large[:, 128:]\n",
    "    msk_large_2 = msk_large[:, 128:]\n",
    "    x_large_2 = roberta_large(input_ids = ids_large_2, attention_mask = msk_large_2)['last_hidden_state']\n",
    "    x_large_2 = x_large_2[:, 0, :]\n",
    "    x_large_2 = Dropout(dropout_rate)(x_large_2)\n",
    "    x_large_2 = Dense(1, activation='linear', kernel_regularizer=regularizers.l2(1e-2))(x_large_2)\n",
    "    \n",
    "    # RoBERTa-Base\n",
    "    ids_base_1 = ids_base[:, :128]\n",
    "    msk_base_1 = msk_base[:, :128]\n",
    "    x_base_1 = roberta_base(input_ids = ids_base_1, attention_mask = msk_base_1)['last_hidden_state']\n",
    "    x_base_1 = x_base_1[:, 0, :]\n",
    "    x_base_1 = Dropout(dropout_rate)(x_base_1)\n",
    "    x_base_1 = Dense(1, activation='linear', kernel_regularizer=regularizers.l2(1e-2))(x_base_1)\n",
    "\n",
    "    ids_base_2 = ids_base[:, 128:]\n",
    "    msk_base_2 = msk_base[:, 128:]\n",
    "    x_base_2 = roberta_base(input_ids = ids_base_2, attention_mask = msk_base_2)['last_hidden_state']\n",
    "    x_base_2 = x_base_2[:, 0, :]\n",
    "    x_base_2 = Dropout(dropout_rate)(x_base_2)\n",
    "    x_base_2 = Dense(1, activation='linear', kernel_regularizer=regularizers.l2(1e-2))(x_base_2)\n",
    "    \n",
    "    ################\n",
    "    #    Output    #\n",
    "    ################\n",
    "\n",
    "    pred = Concatenate(axis=-1)([x_large_1, x_large_2, x_base_1, x_base_2])\n",
    "    coef = Dense(4, activation='softmax', kernel_regularizer=regularizers.l2(1e-2))(pred)\n",
    "    pred = Multiply()([pred, coef])\n",
    "    pred = Dense(1, activation='linear', kernel_initializer=Constant(1), trainable=False)(pred)\n",
    "\n",
    "    ################\n",
    "    #     Model    #\n",
    "    ################\n",
    "\n",
    "    model = Model(inputs=[ids_large, msk_large, ids_base, msk_base], outputs=pred)\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay=0.1, learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss = [tf.keras.losses.MeanSquaredError()],\n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-austria",
   "metadata": {
    "id": "doUqeL2dSfVi",
    "papermill": {
     "duration": 0.011523,
     "end_time": "2021-06-27T22:07:49.300560",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.289037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Training & Evaluating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continental-forth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.327845Z",
     "iopub.status.busy": "2021-06-27T22:07:49.327359Z",
     "iopub.status.idle": "2021-06-27T22:07:49.330590Z",
     "shell.execute_reply": "2021-06-27T22:07:49.330945Z",
     "shell.execute_reply.started": "2021-06-27T22:04:59.854956Z"
    },
    "id": "XUhfn3n3Zh_S",
    "papermill": {
     "duration": 0.018793,
     "end_time": "2021-06-27T22:07:49.331058",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.312265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indian-hollywood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.359382Z",
     "iopub.status.busy": "2021-06-27T22:07:49.358848Z",
     "iopub.status.idle": "2021-06-27T22:07:49.362439Z",
     "shell.execute_reply": "2021-06-27T22:07:49.361989Z",
     "shell.execute_reply.started": "2021-06-27T22:05:00.193631Z"
    },
    "id": "qs8KCzJRSey1",
    "papermill": {
     "duration": 0.019748,
     "end_time": "2021-06-27T22:07:49.362533",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.342785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(max_len, lr, dc, batch, epoch_size, cp_path, seed):\n",
    "    # Prepare sets\n",
    "    train_set = prepare_sets('../input/commonlitreadabilityprize/train.csv', max_len)\n",
    "    test_set = prepare_sets('../input/commonlitreadabilityprize/test.csv', max_len)\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(max_len, lr, dc)\n",
    "    model.summary()\n",
    "  \n",
    "    model.load_weights('../input/ckpt-clrp-1h5/ckpt_clrp.h5')\n",
    "    \n",
    "    # Predict test\n",
    "    test_pred = model.predict([test_set[0], test_set[1], test_set[2], test_set[3]])\n",
    "    \n",
    "    return test_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "light-facing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:07:49.391915Z",
     "iopub.status.busy": "2021-06-27T22:07:49.391429Z",
     "iopub.status.idle": "2021-06-27T22:09:02.180743Z",
     "shell.execute_reply": "2021-06-27T22:09:02.179575Z",
     "shell.execute_reply.started": "2021-06-27T22:05:00.698554Z"
    },
    "id": "rL0KPImHWidE",
    "outputId": "e8572b12-8ebb-4c9a-db21-6a599dcb9fcf",
    "papermill": {
     "duration": 72.806741,
     "end_time": "2021-06-27T22:09:02.180881",
     "exception": false,
     "start_time": "2021-06-27T22:07:49.374140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/initial-roberta-large/roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/initial-roberta-base/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_roberta_large_ids (InputL [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_roberta_large_msk (InputL [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_roberta_base_ids (InputLa [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_roberta_base_msk (InputLa [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 128)          0           input_roberta_large_ids[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 128)          0           input_roberta_large_msk[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (Sli (None, 128)          0           input_roberta_large_ids[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (Sli (None, 128)          0           input_roberta_large_msk[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (Sli (None, 128)          0           input_roberta_base_ids[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (Sli (None, 128)          0           input_roberta_base_msk[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_9 (Sli (None, 128)          0           input_roberta_base_ids[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_10 (Sl (None, 128)          0           input_roberta_base_msk[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 355359744   tf.__operators__.getitem[0][0]   \n",
      "                                                                 tf.__operators__.getitem_1[0][0] \n",
      "                                                                 tf.__operators__.getitem_3[0][0] \n",
      "                                                                 tf.__operators__.getitem_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model_1 (TFRobertaMo TFBaseModelOutputWit 124645632   tf.__operators__.getitem_6[0][0] \n",
      "                                                                 tf.__operators__.getitem_7[0][0] \n",
      "                                                                 tf.__operators__.getitem_9[0][0] \n",
      "                                                                 tf.__operators__.getitem_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (None, 1024)         0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (Sli (None, 1024)         0           tf_roberta_model[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_8 (Sli (None, 768)          0           tf_roberta_model_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_11 (Sl (None, 768)          0           tf_roberta_model_1[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 1024)         0           tf.__operators__.getitem_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 1024)         0           tf.__operators__.getitem_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 768)          0           tf.__operators__.getitem_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 768)          0           tf.__operators__.getitem_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1025        dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            769         dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            769         dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4)            0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            20          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 4)            0           concatenate[0][0]                \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            5           multiply[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 480,008,989\n",
      "Trainable params: 480,008,984\n",
      "Non-trainable params: 5\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "LR = 1e-5\n",
    "DECAY = 0\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "CP_PATH = 'ckpt_clrp.h5'\n",
    "SEED = 123\n",
    "\n",
    "output = train_model(MAX_LEN, LR, DECAY, BATCH_SIZE, EPOCHS, CP_PATH, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pointed-internship",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:09:02.219522Z",
     "iopub.status.busy": "2021-06-27T22:09:02.216353Z",
     "iopub.status.idle": "2021-06-27T22:09:02.342327Z",
     "shell.execute_reply": "2021-06-27T22:09:02.341875Z",
     "shell.execute_reply.started": "2021-06-27T22:06:23.398692Z"
    },
    "id": "fN_Ka61Gkc0S",
    "papermill": {
     "duration": 0.147916,
     "end_time": "2021-06-27T22:09:02.342448",
     "exception": false,
     "start_time": "2021-06-27T22:09:02.194532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "pred_df = {'id': test_df['id'], 'target': output[0].reshape(-1)}\n",
    "pd.DataFrame(pred_df).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "anonymous-savings",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T22:09:02.372576Z",
     "iopub.status.busy": "2021-06-27T22:09:02.371896Z",
     "iopub.status.idle": "2021-06-27T22:09:02.374258Z",
     "shell.execute_reply": "2021-06-27T22:09:02.374628Z"
    },
    "id": "Ykh_fGKxrZ7q",
    "papermill": {
     "duration": 0.018973,
     "end_time": "2021-06-27T22:09:02.374747",
     "exception": false,
     "start_time": "2021-06-27T22:09:02.355774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output[1].save_weights(CP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-methodology",
   "metadata": {
    "id": "epwZ9df36bgj",
    "papermill": {
     "duration": 0.013173,
     "end_time": "2021-06-27T22:09:02.400958",
     "exception": false,
     "start_time": "2021-06-27T22:09:02.387785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 116.756907,
   "end_time": "2021-06-27T22:09:05.166370",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-27T22:07:08.409463",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
