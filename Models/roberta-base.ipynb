{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-airplane",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-12T13:11:46.722749Z",
     "iopub.status.busy": "2021-06-12T13:11:46.722137Z",
     "iopub.status.idle": "2021-06-12T13:11:52.462019Z",
     "shell.execute_reply": "2021-06-12T13:11:52.461082Z"
    },
    "papermill": {
     "duration": 5.759175,
     "end_time": "2021-06-12T13:11:52.462182",
     "exception": false,
     "start_time": "2021-06-12T13:11:46.703007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU, Dense\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Softmax, Conv1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepted-windsor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:11:52.479282Z",
     "iopub.status.busy": "2021-06-12T13:11:52.478787Z",
     "iopub.status.idle": "2021-06-12T13:12:07.067159Z",
     "shell.execute_reply": "2021-06-12T13:12:07.067617Z"
    },
    "papermill": {
     "duration": 14.598367,
     "end_time": "2021-06-12T13:12:07.067763",
     "exception": false,
     "start_time": "2021-06-12T13:11:52.469396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/initial-roberta-base/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('../input/initial-tokenizer-base/tokenizer-base')\n",
    "roberta = TFRobertaModel.from_pretrained('../input/initial-roberta-base/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-swiss",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:12:07.089084Z",
     "iopub.status.busy": "2021-06-12T13:12:07.088434Z",
     "iopub.status.idle": "2021-06-12T13:12:07.091633Z",
     "shell.execute_reply": "2021-06-12T13:12:07.091228Z"
    },
    "papermill": {
     "duration": 0.016211,
     "end_time": "2021-06-12T13:12:07.091746",
     "exception": false,
     "start_time": "2021-06-12T13:12:07.075535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize a bunch of texts\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    tok = tokenizer.batch_encode_plus(texts.tolist(), \n",
    "                                          padding='max_length', \n",
    "                                          max_length=max_len, \n",
    "                                          truncation=True, \n",
    "                                          return_tensors='np')\n",
    "    return tok['input_ids'], tok['attention_mask']\n",
    "\n",
    "# Prepare sets\n",
    "def prepare_sets(is_train_csv, tokenizer, max_len):\n",
    "    # Retrieve dataframes\n",
    "    df = pd.read_csv('../input/commonlitreadabilityprize/' + is_train_csv)\n",
    "    \n",
    "    # Retrieve inputs: IDs and mask\n",
    "    excerpts  = df['excerpt']\n",
    "    tokenized = tokenize_texts(excerpts, tokenizer, max_len)\n",
    "    ids       = tokenized[0]\n",
    "    mask      = tokenized[1]\n",
    "    \n",
    "    # Retrieve labels\n",
    "    targets = None\n",
    "    if is_train_csv == 'train.csv':\n",
    "        targets   = np.array(df['target'])\n",
    "    \n",
    "    return ids, mask, targets, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "russian-asbestos",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:12:07.111304Z",
     "iopub.status.busy": "2021-06-12T13:12:07.110811Z",
     "iopub.status.idle": "2021-06-12T13:12:07.113908Z",
     "shell.execute_reply": "2021-06-12T13:12:07.114642Z"
    },
    "papermill": {
     "duration": 0.015872,
     "end_time": "2021-06-12T13:12:07.114764",
     "exception": false,
     "start_time": "2021-06-12T13:12:07.098892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to transform arrays to tensors\n",
    "def transform_to_tensors(x_train, x_val, y_train, y_val):\n",
    "    \n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(({'input_1': x_train[0], 'input_2': x_train[1]}, y_train))\n",
    "        .shuffle(2048)\n",
    "        .batch(8)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(({'input_1': x_val[0], 'input_2': x_val[1]}, y_val))\n",
    "        .batch(8)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secure-whole",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:12:07.135602Z",
     "iopub.status.busy": "2021-06-12T13:12:07.135068Z",
     "iopub.status.idle": "2021-06-12T13:12:07.138823Z",
     "shell.execute_reply": "2021-06-12T13:12:07.138400Z"
    },
    "papermill": {
     "duration": 0.016925,
     "end_time": "2021-06-12T13:12:07.138925",
     "exception": false,
     "start_time": "2021-06-12T13:12:07.122000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(max_len, learning_rate):\n",
    "    # Inputs: IDs and mask\n",
    "    ids = Input(shape=(max_len,), dtype='int64')\n",
    "    mask = Input(shape=(max_len,), dtype='int64')\n",
    "    \n",
    "    # Processing\n",
    "    x = roberta(input_ids=ids, attention_mask=mask)['last_hidden_state']\n",
    "    x = x[:, 0, :]\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Output\n",
    "    pred = x\n",
    "    \n",
    "    # Model\n",
    "    model = Model(inputs=[ids, mask], outputs=pred)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = learning_rate),\n",
    "                  loss = [tf.keras.losses.MeanSquaredError()],\n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flexible-composite",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:12:07.157471Z",
     "iopub.status.busy": "2021-06-12T13:12:07.156989Z",
     "iopub.status.idle": "2021-06-12T13:12:07.160624Z",
     "shell.execute_reply": "2021-06-12T13:12:07.160223Z"
    },
    "papermill": {
     "duration": 0.014433,
     "end_time": "2021-06-12T13:12:07.160731",
     "exception": false,
     "start_time": "2021-06-12T13:12:07.146298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-safety",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:12:07.186361Z",
     "iopub.status.busy": "2021-06-12T13:12:07.185843Z",
     "iopub.status.idle": "2021-06-12T13:12:07.189570Z",
     "shell.execute_reply": "2021-06-12T13:12:07.189140Z"
    },
    "papermill": {
     "duration": 0.021472,
     "end_time": "2021-06-12T13:12:07.189668",
     "exception": false,
     "start_time": "2021-06-12T13:12:07.168196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(max_len, lr, batch, epoch_size, cp_path, seed):\n",
    "    # Prepare sets\n",
    "    train_set = prepare_sets('train.csv', tokenizer, max_len)\n",
    "    test_set = prepare_sets('test.csv', tokenizer, max_len)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(max_len, lr)\n",
    "    model.summary()\n",
    "    \n",
    "    # Randomize\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(cp_path, \n",
    "                                                    monitor = 'val_root_mean_squared_error', \n",
    "                                                    verbose = 2, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "    \n",
    "    reduce_lr=ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\",\n",
    "                                factor=0.2,\n",
    "                                patience=5,\n",
    "                                min_lr=1e-8)\n",
    "    \n",
    "    early_stopping=EarlyStopping(monitor=\"val_root_mean_squared_error\",\n",
    "                                 min_delta=0,\n",
    "                                 patience=5,\n",
    "                                 verbose=2,\n",
    "                                 mode=\"min\",\n",
    "                                 restore_best_weights=True)\n",
    "    \n",
    "    # Use k-fold CV\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    out_of_fold_pred = np.zeros((len(train_set[2])))\n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_set[3])):\n",
    "        print('\\nFold', fold+1, '*'*50)\n",
    "        \n",
    "        sets = transform_to_tensors(x_train = (train_set[0][trn_ind], train_set[1][trn_ind]),\n",
    "                                    x_val = (train_set[0][val_ind], train_set[1][val_ind]),\n",
    "                                    y_train = train_set[2][trn_ind],\n",
    "                                    y_val = train_set[2][val_ind])\n",
    "    \n",
    "        model.fit(sets[0], \n",
    "                  batch_size=batch, epochs=epoch_size,\n",
    "                  validation_data = sets[1],\n",
    "                  callbacks = [checkpoint, reduce_lr, early_stopping])\n",
    "    \n",
    "        model.load_weights(cp_path)\n",
    "        cv_pred = model.predict(sets[1])\n",
    "        out_of_fold_pred[val_ind] = cv_pred.reshape(-1)\n",
    "    \n",
    "    cv_rmse = np.sqrt(mean_squared_error(train_set[2], out_of_fold_pred))\n",
    "    print('Out-of-fold Root Mean Square Error is:', cv_rmse)\n",
    "    \n",
    "    # Predict test\n",
    "    test_pred = model.predict([test_set[0], test_set[1]], test_set[2])\n",
    "    \n",
    "    return test_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developing-narrative",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:12:07.210787Z",
     "iopub.status.busy": "2021-06-12T13:12:07.210286Z",
     "iopub.status.idle": "2021-06-12T14:11:24.862037Z",
     "shell.execute_reply": "2021-06-12T14:11:24.861596Z"
    },
    "papermill": {
     "duration": 3557.664943,
     "end_time": "2021-06-12T14:11:24.862167",
     "exception": false,
     "start_time": "2021-06-12T13:12:07.197224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 124645632   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 768)          0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 124,646,401\n",
      "Trainable params: 124,646,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Fold 1 **************************************************\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 102s 308ms/step - loss: 0.8231 - root_mean_squared_error: 0.9021 - val_loss: 0.3126 - val_root_mean_squared_error: 0.5591\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.55911, saving model to cp_roberta_base.h5\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.3730 - root_mean_squared_error: 0.6104 - val_loss: 0.2846 - val_root_mean_squared_error: 0.5335\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.55911 to 0.53346, saving model to cp_roberta_base.h5\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.3027 - root_mean_squared_error: 0.5500 - val_loss: 0.4526 - val_root_mean_squared_error: 0.6728\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.53346\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 84s 298ms/step - loss: 0.2437 - root_mean_squared_error: 0.4934 - val_loss: 0.4168 - val_root_mean_squared_error: 0.6456\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.53346\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.2018 - root_mean_squared_error: 0.4490 - val_loss: 0.5653 - val_root_mean_squared_error: 0.7519\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.53346\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.1936 - root_mean_squared_error: 0.4399 - val_loss: 0.3184 - val_root_mean_squared_error: 0.5642\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.53346\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 84s 298ms/step - loss: 0.1544 - root_mean_squared_error: 0.3925 - val_loss: 0.3487 - val_root_mean_squared_error: 0.5905\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.53346\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Fold 2 **************************************************\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.2721 - root_mean_squared_error: 0.5216 - val_loss: 0.2803 - val_root_mean_squared_error: 0.5294\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from 0.53346 to 0.52942, saving model to cp_roberta_base.h5\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.2559 - root_mean_squared_error: 0.5059 - val_loss: 0.3234 - val_root_mean_squared_error: 0.5687\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.52942\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.2266 - root_mean_squared_error: 0.4761 - val_loss: 0.3752 - val_root_mean_squared_error: 0.6126\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.52942\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.2138 - root_mean_squared_error: 0.4623 - val_loss: 0.2842 - val_root_mean_squared_error: 0.5331\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.52942\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.2000 - root_mean_squared_error: 0.4472 - val_loss: 0.2292 - val_root_mean_squared_error: 0.4788\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.52942 to 0.47878, saving model to cp_roberta_base.h5\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 84s 297ms/step - loss: 0.1744 - root_mean_squared_error: 0.4176 - val_loss: 0.3847 - val_root_mean_squared_error: 0.6202\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.47878\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1718 - root_mean_squared_error: 0.4145 - val_loss: 0.3546 - val_root_mean_squared_error: 0.5955\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.47878\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1626 - root_mean_squared_error: 0.4033 - val_loss: 0.3857 - val_root_mean_squared_error: 0.6210\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.47878\n",
      "Epoch 9/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1587 - root_mean_squared_error: 0.3983 - val_loss: 0.3099 - val_root_mean_squared_error: 0.5567\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.47878\n",
      "Epoch 10/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1457 - root_mean_squared_error: 0.3817 - val_loss: 0.3955 - val_root_mean_squared_error: 0.6289\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.47878\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Fold 3 **************************************************\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.2003 - root_mean_squared_error: 0.4476 - val_loss: 0.2334 - val_root_mean_squared_error: 0.4832\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.47878\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1862 - root_mean_squared_error: 0.4315 - val_loss: 0.2639 - val_root_mean_squared_error: 0.5137\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.47878\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1843 - root_mean_squared_error: 0.4293 - val_loss: 0.2171 - val_root_mean_squared_error: 0.4659\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.47878 to 0.46593, saving model to cp_roberta_base.h5\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1849 - root_mean_squared_error: 0.4300 - val_loss: 0.2655 - val_root_mean_squared_error: 0.5153\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 85s 301ms/step - loss: 0.1872 - root_mean_squared_error: 0.4327 - val_loss: 0.2303 - val_root_mean_squared_error: 0.4799\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1723 - root_mean_squared_error: 0.4151 - val_loss: 0.2781 - val_root_mean_squared_error: 0.5273\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1725 - root_mean_squared_error: 0.4153 - val_loss: 0.2319 - val_root_mean_squared_error: 0.4816\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1680 - root_mean_squared_error: 0.4099 - val_loss: 0.2299 - val_root_mean_squared_error: 0.4795\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.46593\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "Fold 4 **************************************************\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1834 - root_mean_squared_error: 0.4283 - val_loss: 0.2362 - val_root_mean_squared_error: 0.4860\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1801 - root_mean_squared_error: 0.4243 - val_loss: 0.2483 - val_root_mean_squared_error: 0.4983\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1762 - root_mean_squared_error: 0.4197 - val_loss: 0.2584 - val_root_mean_squared_error: 0.5083\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 85s 301ms/step - loss: 0.1789 - root_mean_squared_error: 0.4229 - val_loss: 0.2472 - val_root_mean_squared_error: 0.4972\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1827 - root_mean_squared_error: 0.4274 - val_loss: 0.2354 - val_root_mean_squared_error: 0.4852\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1808 - root_mean_squared_error: 0.4252 - val_loss: 0.2466 - val_root_mean_squared_error: 0.4966\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.1775 - root_mean_squared_error: 0.4213 - val_loss: 0.2516 - val_root_mean_squared_error: 0.5016\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1798 - root_mean_squared_error: 0.4240 - val_loss: 0.2434 - val_root_mean_squared_error: 0.4933\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 9/10\n",
      "284/284 [==============================] - 85s 298ms/step - loss: 0.1771 - root_mean_squared_error: 0.4208 - val_loss: 0.2410 - val_root_mean_squared_error: 0.4909\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 10/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1833 - root_mean_squared_error: 0.4281 - val_loss: 0.2588 - val_root_mean_squared_error: 0.5087\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.46593\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Fold 5 **************************************************\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1774 - root_mean_squared_error: 0.4212 - val_loss: 0.2438 - val_root_mean_squared_error: 0.4937\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1817 - root_mean_squared_error: 0.4263 - val_loss: 0.2552 - val_root_mean_squared_error: 0.5052\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1833 - root_mean_squared_error: 0.4281 - val_loss: 0.2483 - val_root_mean_squared_error: 0.4983\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 85s 300ms/step - loss: 0.1860 - root_mean_squared_error: 0.4312 - val_loss: 0.2515 - val_root_mean_squared_error: 0.5015\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 85s 299ms/step - loss: 0.1822 - root_mean_squared_error: 0.4268 - val_loss: 0.2604 - val_root_mean_squared_error: 0.5103\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.46593\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 85s 301ms/step - loss: 0.1791 - root_mean_squared_error: 0.4231 - val_loss: 0.2586 - val_root_mean_squared_error: 0.5085\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.46593\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Out-of-fold Root Mean Square Error is: 0.47894789274403593\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "LR = 1e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "CP_PATH = 'cp_roberta_base.h5'\n",
    "SEED = 2048\n",
    "\n",
    "result = train_model(MAX_LEN, LR, BATCH_SIZE, EPOCHS, CP_PATH, SEED)\n",
    "test_pred = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sharp-significance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T14:11:31.009043Z",
     "iopub.status.busy": "2021-06-12T14:11:31.008106Z",
     "iopub.status.idle": "2021-06-12T14:11:31.203138Z",
     "shell.execute_reply": "2021-06-12T14:11:31.206512Z"
    },
    "papermill": {
     "duration": 3.154992,
     "end_time": "2021-06-12T14:11:31.206739",
     "exception": false,
     "start_time": "2021-06-12T14:11:28.051747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "pred_df = {'id': test_df['id'], 'target': test_pred.reshape(-1)}\n",
    "pd.DataFrame(pred_df).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-croatia",
   "metadata": {
    "papermill": {
     "duration": 3.141423,
     "end_time": "2021-06-12T14:11:37.687349",
     "exception": false,
     "start_time": "2021-06-12T14:11:34.545926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3603.694514,
   "end_time": "2021-06-12T14:11:43.547699",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-12T13:11:39.853185",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
